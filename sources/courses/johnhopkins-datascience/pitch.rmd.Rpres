Clustering
========================================================
author: Daniel Frederico Lins Leite
date: 2017-04-22
autosize: true

Simulation
========================================================

In this presentation we will generate 2000 2D points using two Multivariate Gaussians. Then we will use three different clustering algorithms to try to recognize the original distributions. We will try to find 2, 4 and 6 clusters for each algorithms.

```{r}
library(plotly)
library('MASS')
simulation.g1 <- mvrnorm(n = 1000, c(0,0), diag(2))
simulation.g2 <- mvrnorm(n = 1000, c(-3,-3), diag(c(0.3,10)))
simulation <- rbind(simulation.g1, simulation.g2)
simulation <- data.frame(x = simulation[,1], y = simulation[,2])
```

k Means Clustering
========================================================

```{r echo=FALSE, fig.width = 18}
par(mfrow=c(1,3))
plot(simulation$x, simulation$y, col = kmeans(simulation, 2)$cluster)
plot(simulation$x, simulation$y, col = kmeans(simulation, 4)$cluster)
plot(simulation$x, simulation$y, col = kmeans(simulation, 6)$cluster)
```

Ward Hierarchical Clustering (using Euclidean distance)
===============

```{r echo=FALSE, fig.width = 18}
d <- dist(simulation, method = "euclidean")
fit <- hclust(d, method="ward")
par(mfrow=c(1,3))
plot(simulation$x, simulation$y, col = cutree(fit, k=2))
plot(simulation$x, simulation$y, col = cutree(fit, k=4))
plot(simulation$x, simulation$y, col = cutree(fit, k=6))
```


Gaussian Mixture Model (using BIC)
===============

```{r echo=FALSE, fig.width = 18}
library(mclust)
d <- dist(simulation, method = "euclidean")
fit <- hclust(d, method="ward")
par(mfrow=c(1,3))
plot(simulation$x, simulation$y, col = Mclust(simulation, G=2)$classification)
plot(simulation$x, simulation$y, col = Mclust(simulation, G=4)$classification)
plot(simulation$x, simulation$y, col = Mclust(simulation, G=6)$classification)
```

