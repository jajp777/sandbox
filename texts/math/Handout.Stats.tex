\documentclass[]{book}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}

%opening
\title{Probability and Statistics}
\author{}

\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
	\savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
	\savebox{\mysim}{\hbox{$\sim$}}%
	\mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}


\begin{document}
	
\maketitle
	
\setcounter{tocdepth}{1}
\tableofcontents
\newpage

\chapter{Discrite Distributions}

\section{Generic Formulas}

\subsection{Expected Value}
X is discrete random variable\\
$g : \mathbf{R} \rightarrow \mathbf{R}$\\
$\Omega X = Im(X)$\\

\begin{align}
	E[g(X)] &= \sum_{x \in \Omega X} {P(X=x)*g(x)}
\end{align}

\subsection{Variance}

\begin{align}
	var(g(X)) &= E([g(X) - E(g(X))]^2)\\
	&= \sum_{x\in \Omega X} {[g(X) - E(g(X))]^2*P(X=x)]}\\
	&= \sum_{x\in \Omega X} {[g(X)^2 - 2*g(X)*E(g(X)) + E(g(X))^2]*P(X=x)}\\
	&= \sum_{x\in \Omega X} {g(X)^2*P(X=x)} \\&- \sum_{x\in \Omega X} {2*g(X)*E(g(X))*P(X=x)} \\&+ \sum_{x \in \Omega X} {E(g(X))^2*P(X=x)}\\
	&= \sum_{x\in \Omega X} {g(X)^2*P(X=x)} \\&- 2*E(g(X))*\sum_{x\in \Omega X} {g(X)*P(X=x)} \\&+ E(g(X))^2*\sum_{x \in \Omega X} {P(X=x)}\\
	&= E(g(X)^2) - 2*E(g(X))*E(g(X)) + E(g(X))^2*1\\
	&= E(g(X)^2) - 2*E(g(X))^2 + E(g(X))^2\\
	&= E(g(X)^2) - E(g(X))^2\\
	\qed
\end{align}

\subsection{Covariance Matrix}

\begin{align}
cov(X) &= E[(X-E(X)]^2\\
&= \sum_{x\in \Omega X} {(X-E(X))^2*P(X=x)}\\
&= \sum_{x\in \Omega X} {[X^2-2XE(X)+E(X)^2]*P(X=x)}\\
&= \sum_{x\in \Omega X} {X^2P(X=x)} \\&- \sum_{x\in \Omega X} {2XE(X)*P(X=x)} \\&+ \sum_{x \in \Omega X} {E(X)^2P(X=x)}\\
&= \sum_{x\in \Omega X} {X^2P(X=x)} \\&- 2E(X)*\sum_{x\in \Omega X} {XP(X=x)} \\&+ E(X)^2*\sum_{x \in \Omega X} {P(X=x)}\\
&= E(X^2)-2E(X)E(X)+E(X)^2*1\\
&= E(X^2)-2*E(X)^2+E(X)^2\\
&= E(X^2)-E(X)^2\\
&= E(X^tX)-\mu^t\mu\\
\qed
\end{align}

\subsection{Variance of the Sample Mean}

\begin{align}
Var\left({\overline{X}}\right)&=Var\left( \frac{1}{n}\sum_{i=1}^nX_i\right)\\&= \frac{1}{n^2}Var\left( \sum_{i=1}^nX_i\right)\\&=\frac{1}{n^2}\sum_{i=1}^nVar(X_i), \text{ by independence}\\&= \frac{1}{n^2}\left[Var(X_1)+Var(X_2)+\ldots+Var(X_n) \right]\\&=\frac{1}{n^2}\left[\sigma^2+\sigma^2+\ldots+\sigma^2  \right], \text{ since the }X_i \text{ are identically distributed }\\&= \frac{1}{n^2}(n\sigma^2)\\&=\frac{\sigma^2}{n}
\end{align}

\subsection {Law of Iterated Expectation}
\begin{align}
	E[X] = E[ E[X|Y] ]
\end{align}

\subsection {Law of Total Variance}
\begin{align}
var(X) = E[ var(X|Y) ] + var( E[X|Y] )
\end{align}

\subsection{MSE}

\begin{align}
	\hat{\theta} &= \hat{\theta}(X) \text{ Random Variable}\\
	E[\hat{\theta}] &= \text{constant}\\
	\theta &= \text{true value, constant}\\
	\\
	MSE(\hat{\theta}) &= E[(\hat{\theta} - \theta)^2]\\
	&= E[(\hat{\theta} - E[\hat{\theta}] + E[\hat{\theta}]) - \theta)^2]\\
	&= E[([\hat{\theta} - E[\hat{\theta}]] + [E[\hat{\theta}] - \theta])^2]\\
	&= E[(A + B)^2]\\
	\\
	& A = \hat{\theta} - E[\hat{\theta}]\\
	& B = E[\hat{\theta}] - \theta\\
	\\
	&= E[A^2 + 2AB + B^2]\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2 + 2(\hat{\theta} - E[\hat{\theta}])(E[\hat{\theta}] - \theta) + (E[\hat{\theta}] - \theta)^2]\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + E[2(\hat{\theta} - E[\hat{\theta}])(E[\hat{\theta}] - \theta)] + E[(E[\hat{\theta}] - \theta)^2]\\
	\\	
	& C = E[\hat{\theta}]  - \theta \text{ is a constant}\\
	\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + E[2(\hat{\theta} - E[\hat{\theta}])(C)] + E[C^2]\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + 2CE[\hat{\theta} - E[\hat{\theta}]] + C^2\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + 2C(E[\hat{\theta}] - E[E[\hat{\theta}]]) + C^2\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + 2C(E[\hat{\theta}] - E[\hat{\theta}]) + C^2\\	
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + 2C(0) + C^2\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + C^2\\
	&= E[(\hat{\theta} - E[\hat{\theta}])^2] + (E[\hat{\theta}]  - \theta)^2\\	
	\\
	& var(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2]\\
	& bias(\hat{\theta}, \theta) = E[\hat{\theta}]  - \theta\\
	\\
	&= var(\hat{\theta}) + (bias(\hat{\theta}, \theta))^2\\	
\end{align}

\section {Uniform Random Variable}

\section {Bernoulli Distribution}

The Bernoulli Distribution is a special case of the Binomial Distribution, where $$n=1$$.

\subsection{PMF}
\begin{align}
P(X=k) &= \binom{1} {k} p^k (1-p)^{1-k}\\
&=p^k (1-p)^{n-k}
\end{align}

\subsection {Expected Value}
\begin{align}
E(x) &= \sum_{k \geqslant 1} [\binom{n}{k} p^k (1-p)^{n-k}] * k\\
&= np && \text{see Binomial Distribution E[X]}\\
&= 1*p\\
&= p
\end{align}

\subsection {Variance}
\begin{align}
Var(X) &= np*(1-p)\\
&= p*(1-p)\\
\end{align}

\subsection {Likelihood of IID Bernoulli}

\begin{align}
x_i \overset{iid}{\sim} Bernoulli(p)\\
L(x_i|p)&=p(x_1,x_2,...,x_n|p)\\&=\prod_{n=1}^np(x_i|p)\\
&=p^S*(1-p)^{n-S}
\end{align}

\subsection {Maximun Likelihood}

\begin{align}
\frac{d[L(x_i|p)]}{dp} &=\frac{d[p^S*(1-p)^{n-S}]}{dp}\\
\frac{d[log(L(x_i|p))]}{dp} &=\frac{d[log(p^S*(1-p)^{n-S})]}{dp}\\
&= \frac{d}{dp}*[log(p^S*(1-p)^{n-S})]\\
&= \frac{d}{dp}[log(p^S)+log((1-p)^{n-S})]\\
&= \frac{d}{dp}[S*log(p)+(n-S)*log(1-p)]\\
&= S*\frac{d}{dp}[log(p)]+(n-S)*\frac{d}{dp}[log(1-p)]\\
&= S*[\frac{1}{p}]+(n-S)*\frac{d}{dp}[log(1-p)] && \text{chain rule}\\
&= S*\frac{1}{p}+(n-S)*\frac{1}{p-1}\\
&= \frac{S}{p}+\frac{n-S}{p-1}\\
&= \frac{S*(p-1)}{p*(p-1)}+\frac{p*(n-S)}{p*(p-1)}\\
&= \frac{S*(p-1)+p*(n-S)}{p*(p-1)}\\
&= \frac{S*p-S+p*n-p*S}{p*(p-1)}\\
&= \frac{-S+p*n}{p*(p-1)}\\
0 &= \frac{-S+p*n}{p*(p-1)}\\
0*(p*(p-1)) &= -S+p*n\\
0 &= -S+p*n\\
S &= p*n\\
\frac{S}{n} &= p\\
p &= \frac{S}{n}\\
\end{align}

\subsection{MGF}

\begin{align}
	M(t) &= E[e^{tX}]\\
	&= (1-p)e^{t*0} + pe^{t*1}\\
	&= (1-p) + pe^{t}\\
\end{align}

\subsubsection{E[X] using MGF}

\begin{align}
E[X] &= \frac{d^1}{dt^1}[M(t)](0)\\
\\
M(t) &= (1-p) + pe^{t}\\
\\
E[X] &= \frac{d^1}{dt^1}[(1-p) + pe^{t}](0)\\
&= [\frac{d^1}{dt^1}[(1-p)] + \frac{d^1}{dt^1}[pe^{t}]](0)\\
&= [0 + \frac{d^1}{dt^1}[pe^{t}]](0)\\
&= [\frac{d^1}{dt^1}[pe^{t}]](0)\\
&= [pe^{t}](0)\\
&= [pe^{0}]\\
&= p*1\\
&= p\\
\end{align}

\subsubsection{$E[X^2]$ using MGF}

\begin{align}
E[X^2] &= \frac{d^2}{dt^2}[M(t)](0)\\
\\
M(t) &= (1-p) + pe^{t}\\
\\
E[X^2] &= \frac{d^2}{dt^2}[(1-p) + pe^{t}](0)\\
&= \frac{d^1}{dt^1}[pe^{t}](0) && \text{see E[X] using MGF}\\
&= [pe^{t}](0)\\
&= [pe^{0}]\\
&= p*1\\
&= p\\
\end{align}

And these values can be checked calculating the $var[X]$.

\begin{align}
	var[X] &= E[X^2] - E[X]^2\\
	&= p - p^2\\
	&= p*(1-p)\\
	\qed
\end{align}

\subsubsection{$E[X^n]$ using MGF}

\begin{align}
E[X^2] &= \frac{d^2}{dt^2}[M(t)](0)\\
\\
M(t) &= (1-p) + pe^{t}\\
\\
E[X^2] &= \frac{d^2}{dt^2}[(1-p) + pe^{t}](0)\\
\end{align}

\section{Binomial Distribution}

\subsection{PMF}
\begin{align}
P(X=k) &= \binom{n} {k} p^k (1-p)^{n-k}
\end{align}

%prove that the summation is 1
%prove the expected value
%\paragraph{https://proofwiki.org/wiki/Expectation_of_Binomial_Distribution}

\subsection {Expected Value}

$E[g(X)]$ when $g(X) = X$.

\begin{align}
E(X) &= \sum_{k \geqslant 0}P(x=k)*k\\
&= \sum_{k \geqslant 0}[\binom{n} {k} p^k (1-p)^{n-k}] * k\\
\end{align}

when $$k=0$$, the formula $$[\binom{n} {k} p^k (1-p)^{n-k}] * k = [\binom{n} {0} p^k (1-p)^n] * 0 = 0$$, so the index of the summation can be increased by 1.

\begin{align}
E(X) &= \sum_{k \geqslant 1} \binom{n}{k} p^k (1-p)^{n-k} * k\\
&= \sum_{k \geqslant 1} \frac{n}{k} * \binom{n-1}{k-1} p^k (1-p)^{n-k} * k && \text{see BinomialCoefficient}\\
&= \sum_{k \geqslant 1} \frac{n*k}{k} * \binom{n-1}{k-1} p^k (1-p)^{n-k}\\
&= \sum_{k \geqslant 1} n * \binom{n-1}{k-1} p^k (1-p)^{n-k}\\
&= \sum_{k \geqslant 1} n*p * \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k}\\
&= np * \sum_{k \geqslant 1} \binom{n-1}{k-1} p^{k-1} (1-p)^{n-k}\\
u = n-1\\
z = k-1\\
u-z&=(n-1)-(k-1)\\
&=n-1-k+1\\
&=n-k\\
k>1&=(z+1)>1\\
&=z>0\\
&= np * \sum_{z>0} \binom{u}{z} p^{z} (1-p)^{u-z}\\
&= np * 1 && see BinomialDistributionProofEquals1\\
&= np\\
\qedsymbol
\end{align}

\subsection {Variance}

\begin{align}
	Var(X) &= E(X^2) - E(X)^2 && \text{see Variance}\\
	&= \sum_{k \geqslant 0} {[\binom{n} {k} p^k (1-p)^{n-k}]*k^2} - np && \text{see Binomial Expected Value}
\end{align}

when $$k=0$$, the formula $$[\binom{n} {k} p^k (1-p)^{n-k}] * k = [\binom{n} {0} p^k (1-p)^n] * 0 = 0$$, so the index of the summation can be increased by 1.

\begin{align}
&= \sum_{k \geqslant 1} {[\binom{n} {k} p^k (1-p)^{n-k}]*k^2} - (np)^2\\
&= \sum_{k \geqslant 1} {\frac{n}{k} [\binom{n-1} {k-1} p^k (1-p)^{n-k}]*k^2} - (np)^2\\
&= \sum_{k \geqslant 1} {\frac{n*k^2}{k} [\binom{n-1} {k-1} p^k (1-p)^{n-k}]} - (np)^2\\
&= \sum_{k \geqslant 1} {[nk*\binom{n-1} {k-1} p^k (1-p)^{n-k}]} - (np)^2\\
&= \sum_{k \geqslant 1} {[nkp*\binom{n-1} {k-1} p^{k-1} (1-p)^{n-k}]} - (np)^2\\
&= np*\sum_{k \geqslant 1} {[k*\binom{n-1} {k-1} p^{k-1} (1-p)^{n-k}]} - (np)^2\\
u = n-1\\
z = k-1\\
u-z&=(n-1)-(k-1)\\
&=n-1-k+1\\
&=n-k\\
k >= 1 &= (z+1) >=1\\
&= z >= 0\\
&= np*\sum_{z \geqslant 0} {[(z+1)* \binom{u} {z} p^{z} (1-p)^{u-z}]} - (np)^2\\
&= np*[\sum_{z \geqslant 0} {[z*\binom{u} {z} p^{z} (1-p)^{u-z}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
&= np*[\sum_{z \geqslant 0} {[z*\frac{u}{z}*\binom{u-1} {z-1} p^{z} (1-p)^{u-z}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
&= np*[u*\sum_{z \geqslant 0} {[\binom{u-1} {z-1} p^{z} (1-p)^{u-z}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
\end{align}
\begin{align}
&= np*[up*\sum_{z \geqslant 0} {[\binom{u-1} {z-1} p^{z-1} (1-p)^{u-z}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
&= np*[up*\sum_{z \geqslant 1} {[\binom{u-1} {z-1} p^{z-1} (1-p)^{(u-1)-(z-1)}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
&= np*[up*\sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]} + \sum_{z \geqslant 0} {[\binom{u} {z} p^{z} (1-p)^{u-z}]}] - (np)^2\\
&= np*[up*(p+q)^{u-1} + (p+q)^u] - (np)^2\\
&= np*[(n-1)*p*(p+q)^{n-1-1}+(p+q)^(n-1)] -(np)^2\\
&= np*[(n-1)*p*(p+q)^{n-2}+(p+q)^(n-1)] - (np)^2\\
&= np*([(n-1)*p*(p+q)^{n-2}+(p+q)^(n-1)] - np)\\
&= np*([(n-1)*p+1] - np) && \text{p+q=1}\\
&= np*([(n-1)*p+1] - np)\\
&= np*([np-p +1] - np)\\
&= np*(np-p +1- np)\\
&= np*(-p +1)\\
&= np*(1-p)\\
\qed
\end{align}
 
\section {Geometric Distribution}

\subsection {CDF}

\begin{align*}
	P(X \le x) &=\\
	CDF(X=x) &= \sum_{i=0}^{x}{(1-p)^ip} &\text{by geometric summation}\\
	&= p\frac{1-(1-p)^x}{1-(1-p)}\\
	&= p\frac{1-(1-p)^x}{1-1+p}\\
	&= p\frac{1-(1-p)^x}{p}\\	
	&= 1-(1-p)^x\\	
	\qed\\
	\\
	P(X > x) &= 1 - CDF(X=x)\\
	&= 1 - (1-(1-p)^x)\\
	&= 1 - 1 +(1-p)^x\\	
	&= (1-p)^x\\	
	\qed
\end{align*}

\section {Normal Distribution}

\subsection {Definition}

\begin{align}
pdf(x) &= \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\end{align}

\chapter{Continuous Distributions}

\section{Uniform Distribution}

\subsection{PDF}
\begin{align*}
\int_{a}^{b} {kdx} &= 1\\
&= k\int_{a}^{b} {dx}\\
&= k[x|_{a}^{b}]\\
&= k[b-a]\\	
\\
k[b-a] &= 1\\
k &= \frac{1} {b-a}\\
\end{align*}

\subsection{Expected Value}
\begin{align*}
\int_{a}^{b} {x\big( \frac{1}{b-a} \big)dx} &= \\
&= \int_{a}^{b} {x\big( \frac{1}{b-a} \big)dx}\\
&= \frac{1}{b-a}\int_{a}^{b} {xdx}\\
&= \frac{1}{b-a}\big[ \frac{x^2}{2}|_{a}^{b} \big]\\
&= \frac{1}{b-a}\big[ \frac{b^2}{2} - \frac{a^2}{2} \big]\\
&= \frac{1}{b-a}\big[ \frac{b^2-a^2}{2} \big]\\
&= \frac{1}{b-a}\big[ \frac{(b+a)(b-a)}{2} \big]\\
&= \frac{(b+a)(b-a)}{2(b-a)}\\
&= \frac{b+a}{2}\\
\end{align*}

\subsection{Variance}
\begin{align*}
var(X) &= E[X^2] - E[X]^2\\
&= [\int_{a}^{b} {x^2\big( \frac{1}{b-a} \big)dx}] - (\frac{b+a}{2})^2\\
&= [\frac{1}{b-a}\int_{a}^{b} {x^2dx}] - (\frac{b+a}{2})^2\\
&= [\frac{1}{b-a}\frac{x^3}{3}|_{a}^{b}] - (\frac{b+a}{2})^2\\
&= \frac{1}{b-a}(\frac{b^3}{3}-\frac{a^3}{3}) - (\frac{b+a}{2})^2\\
&= \frac{b^3-a^3}{3(b-a)} - \frac{(b+a)^2}{4}\\
&= \frac{(b-a)(b^2-ab+a^2)}{3(b-a)} - \frac{(b+a)^2}{4}\\
&= \frac{(b^2-ab+a^2)}{3} - \frac{(b+a)^2}{4}\\
&= \frac{4(b^2-ab+a^2)}{12} - \frac{3(b+a)^2}{12}\\
&= \frac{4b^2-4ab+4a^2}{12} - \frac{3(b^2+2ab+a^2)}{12}\\
&= \frac{4b^2-4ab+4a^2}{12} - \frac{3b^2+6ab+3a^2}{12}\\
&= \frac{4b^2-3b^2-4ab-6ab+4a^2-3a^2}{12}\\
&= \frac{b^2-2ab+a^2}{12}\\
&= \frac{(b-a)^2}{12}\\
\end{align*}

\pagebreak
\section {Exponential Distribution}

\subsection{PDF}
\begin{align*}
f_x(x) &= \lambda e^{-\lambda x} && x >= 0
\end{align*}

\subsection{CDF}
\begin{align*}
	CDF(x) = \int_{-\infty}^{x} {f_xdx}\\
	CDF(x) = \int_{-\infty}^{x} {\lambda e^{-\lambda x}dx}\\
	\\
	u = e^{-\lambda x}\\
	\frac{du}{dx} = d[e^{-\lambda x}]\\
	du = d[e^{-\lambda x}]*dx\\
	du = [-\lambda * e^{-\lambda x}]*dx\\
	du = -\lambda e^{-\lambda x}dx\\
	\\
	CDF(x) = \int_{-\infty}^{x} {\lambda e^{-\lambda x}dx}\\
	CDF(x) = -1*\int_{-\infty}^{x} {-1*\lambda e^{-\lambda x}dx}\\
	CDF(x) = -1*\int_{-\infty}^{x} {-\lambda e^{-\lambda x}dx}\\
	CDF(x) = -1*\int_{-\infty}^{x} {du}\\
	CDF(x) = -1*\int_{0}^{x} {du} && \text{by x bounds}\\
	CDF(x) = -1*[u]_{0}^{x}\\
	CDF(x) = -1*[e^{-\lambda x}]_{0}^{x}\\
	CDF(x) = -1*[e^{-\lambda x} - e^{-\lambda 0}]\\
	CDF(x) = -1*[e^{-\lambda x} - e^{0}]\\
	CDF(x) = -1*[e^{-\lambda x} - 1]\\
	CDF(x) = [1 - e^{-\lambda x}]\\
	CDF(x) = 1 - e^{-\lambda x}\\
\end{align*}

\subsection{Expected Value}
\begin{align*}
f_x(x) &= \lambda e^{-\lambda x}\\
\\
E[x] &= \int {x*f_x(x)dx}\\
E[x] &= \int {x*\lambda e^{-\lambda x}dx}\\
\\
	u = x\\
	du = 1*dx\\
	v = e^{-\lambda x}\\
	dv = e^{-\lambda x}*-\lambda * dx\\
	dv = -\lambda e^{-\lambda x}dx\\
	\\
E[x] &= \int {[x] [\lambda e^{-\lambda x}dx]}\\
E[x] &= -1*\int {[x] [-1* \lambda e^{-\lambda x}dx]}\\
E[x] &= -1*\int {[x] [-\lambda e^{-\lambda x}dx]}\\
E[x] &= -1*\int {udv}\\
E[x] &= -1*[uv|_{-\infty}^{\infty} - \int {vdu}]\\
E[x] &= -1*[uv|_{0}^{\infty} - \int {vdu}] && \text{because x bounds}\\
E[x] &= -1*[[xe^{-\lambda x}]_{0}^{\infty} - \int {e^{-\lambda x}dx}]\\
\end{align*}
\begin{align*}
[xe^{-\lambda x}]_{0}^{\infty} &= \lim_{x->\infty}{[xe^{-\lambda x} - (0)e^{-\lambda (0)}]}\\
&= \lim_{x->\infty}{[xe^{-\lambda x} + 0e^{0}]}\\
&= \lim_{x->\infty}{[xe^{-\lambda x}]}\\
&= 0\\
\\
E[x] &= -1 * - \int {e^{-\lambda x}dx}\\
E[x] &= \frac{1}{\lambda} \int {\lambda e^{-\lambda x}dx}\\
E[x] &= \frac{1}{\lambda}\\
\end{align*}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{Random Variables Relationships}
\end{figure}


\end{document}